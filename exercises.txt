
I have now been able to finish my week one studies with python. 
 
 i would get Exercises here to do to perfect my craft 


ðŸŽ¯ Practice Exercises
Exercise 1: Word Counter
python# Count how many times each word appears
sentence = "the quick brown fox jumps over the lazy dog and the fox ran away"

# Create a dictionary: word â†’ count
# Then print the 3 most common words

Exercise 3: Set Operations
python# Given two lists of email addresses:
list1 = ["alice@email.com", "bob@email.com", "charlie@email.com"]
list2 = ["bob@email.com", "david@email.com", "alice@email.com"]

# Find:
# 1. All unique emails
# 2. Emails in both lists
# 3. Emails only in list1
# 4. Emails in exactly one list
Exercise 4: Performance Challenge
python# You have a list of 10,000 product IDs
products = list(range(10000))

# Write two functions to check if a product exists:
# 1. Using a list (slow)
# 2. Using a set (fast)
#
# Time both and compare!
Exercise 5: Dictionary Comprehension
python# Create a dictionary from 1 to 20 where:
# - Key is the number
# - Value is "even" or "odd", and "prime" or "composite"
# Example: {2: "even prime", 3: "odd prime", 4:
 "even composite"}

 Implement a queue using two stacks
(Hint: Use one stack for enqueue, another for dequeue)

Intermediate Level

Min Stack: Design a stack that supports push, pop, and retrieving the minimum element in O(1) time.
Sliding Window Maximum: Use a deque to find the maximum in each sliding window of size k.

Advanced Level

Evaluate Postfix Expression: Use a stack to evaluate expressions like "3 4 + 2 *" = 14
Design a Browser History System: Use stacks to implement back/forward buttons.


#asnycio
Build async data processor + security vulnerability scan

Exercise 1: Web Scraper Comparison
Build the same web scraper using:

Sequential requests

Threading

Async/await

Time each approach and compare results.

Exercise 2: Image Processor
Process multiple images using:

Sequential processing

Multiprocessing

ThreadPoolExecutor vs ProcessPoolExecutor

Exercise 3: Mixed Workload
Create a program that handles both:

I/O: Fetching data from URLs

CPU: Processing that data

Use the right tool for each part.



Intermediate:
Build a program that converts between JSON and CSV formats

Create a backup script that copies all .txt files from one directory to another

Build a simple logging system that appends messages with timestamps to a file

Advanced:
Create a configuration manager that reads settings from JSON but can override them with environment variables

Build a file organizer that sorts files into folders based on their extensions

Create a data migration script that updates the format of old JSON files to a new version

Log Parser: Extract all timestamps from logs like: [2024-01-15 14:30:22] Error occurred

Data Cleaner: Remove all non-alphanumeric characters from a string except spaces

URL Extractor: Find all URLs in text and separate protocol, domain, and path
